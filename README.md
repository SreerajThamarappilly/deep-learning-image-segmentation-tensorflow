# deep-learning-image-segmentation-tensorflow

This project implements a deep learning-based image segmentation pipeline using TensorFlow and U-Net architecture. The system performs semantic segmentation on images and includes an end-to-end pipeline from data preprocessing to model training, evaluation, and deployment via a Flask REST API.

## Directory Structure

```bash
deep-learning-image-segmentation-tensorflow/
├── README.md               # Project documentation
├── requirements.txt        # List of Python dependencies
├── saved_model.keras       # Trained model saved in Keras format
├── saved_images/           # Directory for saving input and output images
├── data/                   # Directory containing raw and preprocessed data
│   ├── images/             # Directory for raw input images
│   ├── annotations/        # Directory for raw annotation files
│   ├── preprocessed/       # Directory for preprocessed images and masks
├── scripts/                # Contains all Python scripts for the project
│   ├── data_preprocessing.py  # Script for preparing and preprocessing data
│   ├── model.py               # Script defining the U-Net model architecture
│   ├── train.py               # Script for training the U-Net model
│   ├── evaluate.py            # Script for evaluating the trained model
│   └── app.py                 # Flask REST API script for serving the model
```

- **README.md**: Project documentation.
- **requirements.txt**: List of Python dependencies.
- **saved_model.keras**: Contains the trained U-Net model saved in Keras format.
- **saved_images/**: Stores input images and their corresponding output segmentation masks processed via the REST API.
- **data/**: Main directory for datasets and preprocessed data.
- **data/images/**: Stores raw input images for training and testing.
- **data/annotations/**: Contains annotation files (e.g., COCO JSON) used for segmentation.
- **data/preprocessed/**: Stores preprocessed images and binary masks generated by data_preprocessing.py.
- **scripts/**: Directory containing Python scripts for each stage of the pipeline.
- **scripts/data_preprocessing.py**: Reads raw images & annotations, resizes & normalizes data and generates binary segmentation masks.
- **scripts/model.py**: Defines the U-Net architecture using TensorFlow and Keras.
- **scripts/train.py**: Trains the U-Net model, saves the best model checkpoint based on validation loss.
- **scripts/evaluate.py**: Evaluates the trained model on test data, computes loss and accuracy metrics.
- **scripts/app.py**: Implements a Flask REST API for serving the model, accepts image files via POST requests and returns segmentation masks in base64 format.

## Running the Application Locally

- **Installation of Libraries**:

```bash
pip install -r requirements.txt
```

- **Prepare Data**:

```bash
python scripts/data_preprocessing.py
```

- **Train the Model**:

```bash
python scripts/train.py
```

- **Evaluate the Model**:

```bash
python scripts/evaluate.py
```

## Testing the Application

```bash
curl -X POST "http://localhost:5000/segment" \
-F "file=@1.jpg"
```

## License

*This project is licensed under the MIT License.*
